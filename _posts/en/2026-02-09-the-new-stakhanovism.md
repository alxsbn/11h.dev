---
layout: post
title: "The New Stakhanovism"
date: 2026-02-09
categories: ai work philosophy organization series
excerpt: 'The Stakhanovite movement celebrated model workers to raise quotas for everyone else. AI-native companies are running the same playbook.'
header_image: "https://upload.wikimedia.org/wikipedia/commons/thumb/f/f1/Stakhanov.JPG/1600px-Stakhanov.JPG"
header_image_alt: "Alexei Stakhanov, Soviet coal miner and hero of the Stakhanovite movement"
header_image_credit: "Soviet Archives"
header_image_credit_url: "https://commons.wikimedia.org/wiki/File:Stakhanov.JPG"
header_image_source: "Wikimedia Commons"
header_image_source_url: "https://commons.wikimedia.org"
ref: the-new-stakhanovism
redirect_from:
  - /2026/02/09/the-new-stakhanovism/
lang: en
---

## The fable

In 1935, Alexei Stakhanov extracted 102 tonnes of coal in a single shift, 14 times the norm. The Soviet Union made him a hero, and the Stakhanovite movement was born. Model workers surpassed quotas, proving the system worked.

What the propaganda didn't mention were the teams that cleared the way before him, removed obstacles, and prepared his tools. He wasn't alone, he was *staged*.

The staging served two purposes: legitimize the system, and raise quotas for everyone else. "If Stakhanov can do it, why can't you?"

In June 2025, Elena Verna, Head of Growth at Lovable, publishes [The Rise of the AI-Native Employee](https://www.elenaverna.com/p/the-rise-of-the-ai-native-employee). She describes a radical model: small teams with no process, no handoffs and no middle management, where AI is the default tool. Build, don't coordinate, with 35 people for $80M ARR. The #AINativeEmployeeEra.

Eight months later, Lovable puts Lazar Jovanovic on [Lenny's Podcast](https://www.lennysnewsletter.com/p/getting-paid-to-vibe-code), the largest product podcast in tech. He's their "first professional vibe coder": no coding background, he ships production-quality products using only AI. The thesis made flesh.

Meanwhile, Lovable lists dozens of open positions: GTM Operations, Recruitment Coordinator, Customer Success Manager, Enterprise Account Executive, and Security Engineer with "5+ years in cloud-native environments." Aren't these exactly the roles Elena described as going extinct?

Lazar is not an engineer and doesn't build Lovable's systems; he shows what's achievable with AI. Behind his demos, there's infrastructure maintained by platform engineers, security handled by specialists, enterprise deals closed by experienced AEs, and a GTM machine staffed by seasoned operators. Lovable is the most visible case of a broader pattern. Klarna replaced 700 customer service agents with AI and made it a press release. Duolingo cut contractors after expanding its AI capabilities and framed it as evolution. The fable is always the same: the system works, and the human is optional.

This doesn't refute the thesis, a comparable SaaS at this stage might employ a thousand people, and Lovable may genuinely be building a leaner model. But there's a gap between a transition and a fable. When a company's actual headcount diverges that far from the narrative it projects, you're not describing the future, you're curating it.

The parallel isn't political, nobody went to the gulag. It's *narrative*: a model worker made visible, an infrastructure made silent, and a norm set for everyone else.

## The zeal

But the fable only works if we forget what it compresses. And what gets compressed first is the nature of human work itself.

Take a worker on an automotive assembly line. He needs to screw a bolt into a thread. The bolt doesn't fit. He tries a second one, then a third, none of them go in. The procedure says to stop the line. But stopping the line costs tens of thousands of euros per minute. So he runs to grab a bucket of oil, dips the bolts in, and forces them in one by one. The line doesn't stop. The product ships, and no one knows. This is [living work](/en/2026/01/01/the-cheating-that-makes-the-world-run/).

The deviation *is* the work, and the deviation is a form of zeal, not obedience to the instruction but loyalty to the outcome. This is productive zeal, the kind that says: I know what was asked, but here's what actually needs to happen.

There's another kind: the unnecessary meeting convened out of process loyalty, the political workaround, the ritual code review no one actually reads, and the weekly status report that disappears into the void. This is parasitic zeal, devotion to the ritual of work rather than its substance.

An AI agent strips both in the same gesture. It has no body, no friction with the real. When the bolt doesn't fit, it escalates, blocks, or hallucinates a solution. It doesn't dip anything in oil, it doesn't *cheat*. Every deviation an agent makes was anticipated by its designer, while productive zeal, by definition, exceeds what anyone pre-configured. But the agent also doesn't convene pointless meetings. It removes productive deviance and parasitic deviance identically, because from inside a system prompt, they look the same.

"No process, no handoffs, just build" is the [work-to-rule](/en/2026/01/01/the-cheating-that-makes-the-world-run/) of AI, not obedience to slow things down but obedience to speed things up. In both cases, the space where human judgment corrects the gap between instruction and reality gets compressed.

When the agent automates everything, who guarantees zeal? Not the parasitic kind, but the productive kind, the kind that senses the bolt doesn't fit before the product ships.

Someone has to decide what the agent knows, what it ignores, and where it stops. Someone configures the frame, and that someone may already be the most important person in the building.

## The compression

The Stakhanovite parallel goes beyond narrative. The movement also served a specific economic function by raising quotas. If Stakhanov could do 14x the norm, the norm could go up. Every worker was now measured against the model, and the productivity wasn't redistributed, it was *extracted*.

The AI-native narrative runs the same logic with different packaging. When a company demonstrates that one person with AI can do the work of four, two things can happen: produce four times more, or fire three out of four.

Both happen, but the dominant narrative in the AI-native discourse rewards extraction and invisibilizes redistribution. The model worker goes on a podcast, while the people whose roles became "redundant" update their LinkedIn in silence.

This is the compression. Not the compression of execution time (that part is real, and often welcome), but the compression of the workforce into a tighter, faster, more surveilled unit where the norm has been reset by a demo on a podcast. Where "if Lazar can do it" quietly becomes "why do we need you?"

But the compression doesn't stop at extraction. It also recalibrates what everyone considers a normal pace. In a team, the person who uses AI effectively is noticeably more productive than the one who doesn't. Everyone develops their own formula, their own AI setup to produce beyond what the old pace allowed. Output goes up, expectations follow, and end-of-day fatigue increases because the bar is higher. At some point, a choice will have to be made: the new Stakhanov, the one who produces more with their AI setup, will be preferred over the other. The mechanic is exactly the same: the model worker recalibrates the norm for everyone else.

In this context, Lazar raises a radical question. He has no engineering background and doesn't build Lovable's systems; he ships products with AI tools. But if you take him seriously, what's the remaining argument for the certified engineer stacking increasingly complex architectures? It's not settled, but the question is on the table.

And the person who configures the agent that sets the pace? That person decides, in practice, what "normal productivity" looks like. Not by setting quotas, but by setting context.

## The banality of configuration

This power to configure the frame within which everyone operates is not new. Hannah Arendt identified a specific mechanism, the diligent functionary who executes without reflecting on the finality of what he executes. Not a monster but a bureaucrat, someone who "does engineering" while actually doing ideology. The parallel isn't moral, it's structural.

Call it the banality of configuration. Someone, right now, is configuring the boundaries of an AI agent that every employee interacts with daily. That someone is designing the cognitive architecture of the organization and deciding what's thinkable. This role has no name, no governance, and no audit trail. The fact that it's invisible is precisely what makes it powerful.

This topic goes beyond the scope of this article. I explore it in detail in [Who writes the constitution of machines?](/en/2026/02/18/who-writes-the-constitution-of-machines/), through Claude's Constitution, granted charters, and the invisible governance of enterprise AI deployments.

## What remains open

The Soviet system collapsed not because it failed, but because it [succeeded too well](/en/2026/01/04/collapse-through-obedience/), on paper. Perfect reports, zero truth. The gap between the prescribed and the real grew until the system could no longer see itself.

This article rests on three axes. The fable shows a model worker made visible while the infrastructure stays silent. The zeal reveals what gets lost when you automate without distinguishing productive deviance from parasitic deviance. The compression exposes what rises when the norm is recalibrated by a demo on a podcast. The banality of configuration is the bridge to the next question, that of who designs the frame and by what authority.

The organizations that will survive this transition won't be the fastest, nor the most nostalgic. They'll be the ones that understood that productive zeal (the bolt in the oil, the deviation that saves the product, and the judgment that no system prompt can encode) cannot be automated, only crushed. And that the difference between the slack that wastes and the slack that saves is blurrier than any optimization framework can see.
