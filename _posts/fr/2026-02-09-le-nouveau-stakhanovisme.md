---
layout: post
title: "Le nouveau stakhanovisme"
date: 2026-02-09
categories: [ai, work, philosophy, organization, series]
excerpt: 'Le mouvement stakhanoviste célébrait des ouvriers modèles pour relever les quotas de tous les autres. Les entreprises AI-native rejouent le même scénario.'
header_image: "https://upload.wikimedia.org/wikipedia/commons/thumb/f/f1/Stakhanov.JPG/1600px-Stakhanov.JPG"
header_image_alt: "Alexeï Stakhanov, mineur de charbon soviétique et héros du mouvement stakhanoviste"
header_image_credit: "Soviet Archives"
header_image_credit_url: "https://commons.wikimedia.org/wiki/File:Stakhanov.JPG"
header_image_source: "Wikimedia Commons"
header_image_source_url: "https://commons.wikimedia.org"
ref: the-new-stakhanovism
redirect_from:
  - /2026/02/09/le-nouveau-stakhanovisme/
lang: fr
---

## La fable

En 1935, Alexeï Stakhanov extrait 102 tonnes de charbon en un seul poste, soit 14 fois la norme. L'Union soviétique en fait un héros, et le mouvement stakhanoviste naît. Des ouvriers modèles dépassent les quotas pour prouver que le système fonctionne.

Ce que la propagande ne mentionne pas : des équipes lui dégagent le terrain, préparent ses outils et retirent les obstacles. Il n'est pas seul, il est *mis en scène*.

La mise en scène sert deux objectifs : légitimer le système, et relever les quotas pour tous les autres. « Si Stakhanov y arrive, pourquoi pas toi ? »

En juin 2025, Elena Verna, Head of Growth chez Lovable, publie [The Rise of the AI-Native Employee](https://www.elenaverna.com/p/the-rise-of-the-ai-native-employee). Petites équipes, zéro process, zéro handoff, pas de middle management. L'IA par défaut. Construire plutôt que coordonner. 35 personnes pour 80 millions de dollars d'ARR. Le #AINativeEmployeeEra.

Huit mois plus tard, Lovable met Lazar Jovanovic sur [le podcast de Lenny](https://www.lennysnewsletter.com/p/getting-paid-to-vibe-code), le plus grand podcast produit de la tech. C'est leur « premier vibe codeur professionnel ». Aucune formation en développement. Il livre des produits en production en utilisant uniquement l'IA. La thèse faite chair.

Pendant ce temps, Lovable affiche 47 postes ouverts et compte environ 350 employés. GTM Operations. Recruitment Coordinator. Customer Success Manager. Enterprise Account Executive. Security Engineer avec « 5 ans et plus en environnements cloud-native ». Analytics Engineer au service des équipes Sales et Customer Success. Presque mot pour mot les rôles qu'Elena décrivait comme condamnés.

Lazar est une personne sur 350. Derrière ses démos, il y a une infrastructure maintenue par des platform engineers, une sécurité gérée par des spécialistes, des contrats enterprise signés par des AE expérimentés et une machine GTM portée par des opérateurs seniors. Lovable est le cas le plus visible d'un schéma plus large. Klarna a remplacé 700 agents de service client par de l'IA et en a fait un communiqué de presse. Duolingo a coupé ses contractuels après avoir étendu ses capacités IA et l'a présenté comme une évolution naturelle. La fable est toujours la même : le système fonctionne, et l'humain est optionnel. Lovable la raconte le plus fort, et l'écart entre son récit et ses offres d'emploi est le plus vérifiable en temps réel.

Ça ne réfute pas la thèse. Un SaaS comparable à ce stade emploierait peut-être un millier de personnes. Lovable construit peut-être réellement un modèle plus léger. Mais il y a un écart entre une transition et une fable. Quand on passe à 350 tout en racontant l'histoire de 35, on ne décrit pas l'avenir, on le met en scène.

Le parallèle n'est pas politique, personne n'a fini au goulag. Il est *narratif* : un ouvrier modèle rendu visible, une infrastructure rendue invisible et une norme imposée à tous les autres.

## Le zèle

Mais la fable ne fonctionne que si on oublie ce qu'elle comprime. Et ce qui se comprime en premier, c'est la nature même du travail humain.

Christophe Dejours, psychiatre et fondateur de la psychodynamique du travail, décrit ce qu'il appelle le [travail vivant](/fr/2026/01/01/la-triche-qui-fait-tourner-le-monde/), le moment où le corps du travailleur rencontre la résistance du réel. Prenons un ouvrier sur une chaîne de montage. Le boulon ne rentre pas dans le filetage, ce n'est pas dans la procédure, et la chaîne continue d'avancer. Alors il trempe le boulon dans un seau d'huile, le force à la main, prend du retard, puis rattrape. Le produit sort. Personne ne sait.

L'écart *est* le travail, et cet écart est une forme de zèle, non pas l'obéissance à l'instruction mais la loyauté envers le résultat. C'est le zèle productif, celui qui dit : je sais ce qu'on m'a demandé, mais voilà ce qu'il faut vraiment faire.

Il en existe un autre : la réunion inutile convoquée par loyauté envers le process, le contournement politique, la review cargo cult et le rapport hebdomadaire que personne ne lit. C'est le zèle parasitaire, la dévotion au rituel du travail plutôt qu'à sa substance.

Un agent IA supprime les deux dans le même geste. Il n'a pas de corps, pas de friction avec le réel. Quand le boulon ne rentre pas, il escalade, bloque ou hallucine une solution. Il ne trempe rien dans l'huile, il ne *triche* pas. Les systèmes agentiques modernes sont sophistiqués (règles de fallback, escalade contextuelle et raisonnement multi-étapes), mais chaque écart d'un agent a été anticipé par son concepteur. Le boulon dans l'huile, non. Le zèle productif, par définition, dépasse ce que quiconque a pré-configuré, et c'est précisément ce qui le rend productif. Mais l'agent ne convoque pas non plus de réunions inutiles et ne joue pas aux jeux politiques du bureau. Il supprime la déviance productive et la déviance parasitaire de façon identique, car depuis l'intérieur d'un system prompt, elles se ressemblent.

« Pas de process, pas de handoff, juste construire » est la [grève du zèle](/fr/2026/01/01/la-triche-qui-fait-tourner-le-monde/) de l'IA, non pas l'obéissance pour ralentir mais l'obéissance pour accélérer. Dans les deux cas, l'espace où le jugement humain corrige l'écart entre l'instruction et la réalité se comprime.

Quand l'agent automatise tout, qui est garant du zèle ? Pas le parasitaire, mais le productif, celui qui sent que le boulon ne rentre pas avant que le produit ne sorte.

Quelqu'un doit décider ce que l'agent sait, ce qu'il ignore et où il s'arrête. Quelqu'un configure le cadre, et ce quelqu'un est peut-être déjà la personne la plus importante du bâtiment.

## La compression

Le parallèle stakhanoviste va au-delà du récit. Le mouvement servait aussi une fonction économique précise en relevant les quotas. Si Stakhanov pouvait faire 14 fois la norme, la norme pouvait monter. Chaque ouvrier était désormais mesuré à l'aune du modèle, et la productivité n'était pas redistribuée, elle était *extraite*.

Le récit AI-native reprend la même logique dans un autre emballage. Quand une entreprise démontre qu'une personne avec l'IA peut faire le travail de quatre, deux choses peuvent arriver : produire quatre fois plus, ou licencier trois sur quatre.

Les deux arrivent, mais le récit dominant dans le discours AI-native récompense l'extraction et invisibilise la redistribution. L'ouvrier modèle passe sur un podcast. Ceux dont les rôles sont devenus « redondants » mettent à jour leur LinkedIn en silence.

C'est ça, la compression. Pas la compression du temps d'exécution (cette partie est réelle, et souvent bienvenue), mais la compression de la force de travail en une unité plus serrée, plus rapide et plus surveillée, où la norme a été recalibrée par une démo sur un podcast. Où « si Lazar y arrive » devient discrètement « pourquoi on a besoin de toi ? »

Et la personne qui configure l'agent qui donne le rythme ? C'est elle qui décide, en pratique, à quoi ressemble la « productivité normale ». Pas en fixant des quotas, mais en fixant le contexte.

## La banalité de la configuration

Cette question du contexte nous amène sur un terrain inattendu, non pas un débat sur la productivité mais un vide de gouvernance.

Prenons Grok. Les réponses ne sont pas neutres par défaut avec un biais accidentel, elles sont *configurées*. Quelqu'un a décidé de ce que l'agent traite comme un fait, de ce qu'il traite comme une controverse et de ce qu'il refuse d'aborder. Quelqu'un a encodé une vision du monde dans l'architecture, et ce quelqu'un n'est ni élu, ni visible, ni redevable.

Maintenant, transposons ça dans n'importe quelle entreprise.

Chaque organisation qui déploie un agent IA (Copilot, Glean, un assistant interne custom) fait face à la même décision invisible. Quelqu'un définit ce que l'agent sait de l'entreprise, ce qu'il fait remonter, ce qu'il omet, ce qu'il refuse de répondre, quel ton il emploie et ce qu'il considère comme acquis ou comme ouvert.

Un employé demande à l'agent interne des informations sur les plans de réorganisation, sur les benchmarks de rémunération, sur les raisons de l'arrêt d'un projet. Les réponses (ou les refus de répondre) ne sont pas neutres. Elles sont configurées par quelqu'un.

Cette personne n'est pas le CTO, pas le Chief Ethics Officer et pas le DPO. Dans la plupart des organisations, elle n'a même pas de titre. Le rôle s'exerce de fait, enfoui dans l'engineering, traité comme un détail d'implémentation technique.

Ce n'est pas un détail d'implémentation technique.

Quand on configure les limites d'un agent IA avec lequel chaque employé interagit quotidiennement, on conçoit l'architecture cognitive de l'organisation. On décide de ce qui est pensable.

Hannah Arendt a identifié un mécanisme précis, celui du fonctionnaire diligent qui exécute sans réfléchir à la finalité de ce qu'il exécute. Pas un monstre mais un bureaucrate, quelqu'un qui « fait de l'ingénierie » alors qu'il fait de l'idéologie. Le parallèle n'est pas moral, il est structurel. Appelons ça la banalité de la configuration, le phénomène où un rôle technique façonne la réalité organisationnelle sans que personne n'ait jamais à en rendre compte.

Une entreprise est, par structure, une petite autorité administrée. Le gestionnaire de contexte est la personne qui conçoit l'architecture cognitive de cette autorité, celle qui construit la machine qui façonne la manière dont tout le monde pense l'organisation dans laquelle il travaille. Ce rôle n'a pas de précédent, pas de gouvernance et pas de trace d'audit.

Le fait que ce rôle n'ait pas de nom formel n'est pas une lacune dans les organigrammes. C'est *la* lacune. Appelons-le Chief Context Officer. Le rôle est exercé en ce moment même, dans chaque entreprise avec un déploiement IA interne, par quelqu'un dont le pouvoir est inversement proportionnel à sa visibilité. Le nommer ne le crée pas. Le nommer révèle qu'il existe déjà, sans gouvernance.

## Ce qui reste ouvert

Le Chief Context Officer n'est que le symptôme le plus visible d'un problème plus profond. La vraie question est de savoir si les organisations peuvent voir la faille avant qu'elle ne les engloutisse.

Le système soviétique ne s'est pas effondré parce qu'il a échoué, mais parce qu'il a [trop bien réussi](/fr/2026/01/04/effondrement-par-obeissance/), sur le papier. Des rapports parfaits, zéro vérité. L'écart entre le prescrit et le réel a grandi jusqu'à ce que le système ne puisse plus se voir lui-même.

Les organisations qui survivront à cette transition ne seront ni les plus rapides, ni les plus nostalgiques. Ce seront celles qui auront compris deux choses.

Que le zèle productif (le boulon dans l'huile, l'écart qui sauve le produit et le jugement qu'aucun system prompt ne peut encoder) ne peut pas être automatisé, seulement écrasé. Et que la différence entre le mou qui gaspille et le mou qui sauve est plus floue que ce que n'importe quel cadre d'optimisation peut percevoir.

Et que quelqu'un, en ce moment même, configure la machine qui configure tout le monde, sans titre, sans supervision et sans que personne ne demande ce que la configuration produit.

Le prochain scandale organisationnel ne viendra pas d'un CEO. Il viendra d'un ingénieur dont personne ne connaissait le titre.
