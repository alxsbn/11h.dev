---
layout: post
title: "Transparency for Whom?"
date: 2026-01-15
categories: ai philosophy work series
excerpt: "AI agents can't lie. But who controls what they see? Honesty becomes mandatory for some, optional for others."
header_image: "https://images.unsplash.com/photo-1509822929063-6b6cfc9b42f2?w=1600&q=80"
header_image_alt: "Glass building facade reflecting clouds and sky"
header_image_credit: "Joel Filipe"
header_image_credit_url: "https://unsplash.com/@joelfilip"
header_image_source: "Unsplash"
header_image_source_url: "https://unsplash.com"
---

The [apple pie test](/2026/01/14/the-apple-pie-test/) worked because the cheat was symmetric. Everyone pretended. The consultant pretended to deliver useful documentation. The client pretended to read it. Both sides knew. Both sides benefited from not knowing officially.

AI agents break this symmetry. They can't pretend. They read every page, flag every anomaly, report every discrepancy. They are radically, incorruptibly honest.

But here's the question no one is asking: **honest to whom?**

## The panopticon inverts

When an AI agent reviews your work, it sees everything. Every shortcut, every workaround, every gap between what you said you'd do and what you actually did.

But you don't see the agent's instructions. You don't know what it was told to look for. You don't know what it reports, to whom, in what format. The agent is transparent *about* you, not *to* you.

This is the new asymmetry. Operational workers become legible. Their cheats exposed, their shortcuts flagged. But those who configure the agents remain opaque. They decide what counts as a discrepancy. They define what gets escalated and what gets ignored.

The apple pie consultant could hide recipes because everyone was equally blind. Now, some people see everything while remaining unseen.

## Transparency as power

We tend to think of transparency as democratizing. Sunlight as disinfectant. More visibility, more accountability.

But transparency is never neutral. It's always *directional*. Someone watches. Someone is watched. The question is who's on which side.

A warehouse worker's movements are tracked by AI to the second. A manager's decisions about that AI's parameters are not. A customer service agent's every response is evaluated for compliance. The executive who set the compliance rules operates in strategic ambiguity.

The honest machine creates honest workers. It doesn't necessarily create honest organizations.

## The new hiding places

The old cheats are dying. You can't pretend to have read the documentation when the agent actually reads it. You can't claim to have followed the process when every step is logged.

But new cheats emerge. They just move upstream:

**The prompt is the new back room.** What you tell the agent to do, what you tell it to ignore, what you frame as important—this is where discretion now lives. And prompts aren't audited the way outputs are.

**Complexity is the new opacity.** When a system is too intricate to understand, "the algorithm decided" becomes the new "nobody knows." The agent is honest, but honestly incomprehensible.

**Selection is the new suppression.** You don't hide information from the agent. You just don't ask. The agent can't lie about what it finds. But it only finds what it's pointed at.

The apple pie recipe would be caught instantly. But who decides which documents get reviewed at all?

## Two tiers of honesty

We're building a world with two tiers:

**The observed tier**: Workers whose actions are mediated by agents. Every task logged, every deviation flagged, every shortcut exposed. Radical transparency, mandatory honesty. No room for the informal agreements that made work livable.

**The observer tier**: Those who design, configure, and interpret the agents. Their choices are strategic, not operational. Their discretion is preserved precisely because it's upstream of the system that enforces transparency.

The consultant with his apple pie operated in a world where everyone was equally capable of pretending. The new world has honest machines serving whoever controls them. And that "whoever" doesn't have to be honest at all.

## The transparency test

Here's a simple test: When an organization deploys AI agents for "transparency" or "accountability," ask:

- Who can see the agent's outputs?
- Who defined what the agent looks for?
- Who can modify those definitions?
- Is there an agent watching *that* process?

If the answer is "the same people who benefit from the transparency," you don't have accountability. You have surveillance with extra steps.

Real transparency would mean the agent's instructions are as visible as its findings. The prompt is as auditable as the output. The watchers are as watched as the watched.

## The recipe, relocated

The Z-System guru's apple pie recipe was a test of symmetric blindness. Everyone could pretend, so everyone did.

The new test is different: Can you hide the equivalent of a recipe in the *instructions* to an AI agent? Can you bury biases in prompts, exceptions in configurations, blind spots in scope definitions?

The answer, today, is yes. Easily.

The agent can't cheat. But it can be aimed. And aiming is the new cheating—just for those who hold the compass.
